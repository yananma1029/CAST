{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we evaluate CAST, BERTopic, Top2Vec, LDA, and TopClus on the 20NewsGroups benchmark dataset, which is loaded from the [OCTIS library](https://github.com/MIND-Lab/OCTIS).\n",
    "\n",
    "We use the default LDA implementation from OCTIS and the default parameters for TopClus. For CAST, Top2Vec, and BERTopic, we use the \"all-mpnet-base-v2\" model as an example. Additionally, we fix the UMAP and HDBSCAN parameters across these models to ensure a fair comparison. Each number of topics is evaluated three times, and the scores are averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.dataset.dataset import Dataset # load preprocessed data\n",
    "from CAST import CAST, Candidate\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from evaluation_metrics import TopicCoherence, TopicDiversity\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = Dataset()\n",
    "dataset.fetch_dataset(\"20NewsGroup\")\n",
    "corpus = dataset.get_corpus()\n",
    "documents = [\" \".join(words) for words in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first run, CAST will store the sentence and word embeddings in the `embeddings` folder. For subsequent runs, CAST will load these precomputed embeddings to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'  # you can also use other sentence embedding models\n",
    "\n",
    "params = {\"n_dimensions\": 5, \n",
    "          \"min_cluster_size\": 15, \n",
    "          \"min_count\": 50, \n",
    "          \"self_sim_threshold\": 0.4,\n",
    "          \"candidate_mode\": 'word_level'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nr_topics in [10]:\n",
    "    results = []\n",
    "    for j in range(3):\n",
    "        start = time.time()\n",
    "        model = CAST(documents=documents, model_name=MODEL_NAME, nr_topics=nr_topics, **params)\n",
    "\n",
    "        twords = model.pipeline()\n",
    "        end = time.time()\n",
    "        computation_time = float(end - start)\n",
    "\n",
    "        for cluster_id, keywords in twords.items():\n",
    "            print(f\"Cluster {cluster_id}: {', '.join(keywords)}\") \n",
    "\n",
    "        twords_list = list(twords.values())\n",
    "\n",
    "        scores={}\n",
    "        coherence_metric = TopicCoherence(texts=corpus, topk=10, measure=\"c_npmi\")\n",
    "        coherence_score = coherence_metric.score(twords_list)\n",
    "\n",
    "        diversity_metric = TopicDiversity(topk=10)\n",
    "        diversity_score = diversity_metric.score(twords_list)\n",
    "\n",
    "\n",
    "        print(f\"TC: {coherence_score}; TD: {diversity_score};\")\n",
    "        scores['npmi'] = coherence_score\n",
    "        scores['diversity'] = diversity_score\n",
    "\n",
    "\n",
    "        result = {\n",
    "            \"Model\": MODEL_NAME,\n",
    "            \"Dataset Size\": len(documents),\n",
    "            \"nr_topics\": len(twords_list),\n",
    "            \"Params\": params,\n",
    "            \"Scores\": scores,\n",
    "            \"Computation Time\": computation_time,\n",
    "            \"Topic words\": twords_list,\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "    with open(f\"20News_CAST_{MODEL_NAME.split('/')[1]}_{nr_topics}.json\", \"w\") as file:\n",
    "        json.dump(results, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information: https://github.com/MaartenGr/BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'all-mpnet-base-v2'\n",
    "embedding_model = SentenceTransformer(MODEL_NAME)\n",
    "embeddings = embedding_model.encode(documents, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"embedding_model\": MODEL_NAME,\n",
    "          'min_topic_size': 15,\n",
    "          \"verbose\": True\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nr_topics in [11]:\n",
    "    results = []\n",
    "    for j in range(3):\n",
    "        start = time.time()\n",
    "        model = BERTopic(**params, nr_topics = nr_topics)\n",
    "        topics, probs = model.fit_transform(documents, embeddings)\n",
    "        print (f\"number of topics: {len(set(topics))}\")\n",
    "        end = time.time()\n",
    "        computation_time = float(end - start)\n",
    "\n",
    "        twords = [\n",
    "                    [\n",
    "                        vals[0] for vals in model.get_topic(i)[:10]\n",
    "                    ]\n",
    "                    for i in range(nr_topics-1)\n",
    "                ]\n",
    "        print (f\"the length of twords: {len(twords)}\")\n",
    "        print (twords)\n",
    "        coherence_metric = TopicCoherence(texts=corpus, topk=10, measure=\"c_npmi\")\n",
    "        coherence_score = coherence_metric.score(twords)\n",
    "\n",
    "        diversity_metric = TopicDiversity(topk=10)\n",
    "        diversity_score = diversity_metric.score(twords)\n",
    "        \n",
    "        scores = {}\n",
    "        scores['npmi'] = coherence_score\n",
    "        scores['diversity'] = diversity_score\n",
    "\n",
    "        result = {\n",
    "            \"Model\": MODEL_NAME,\n",
    "            \"Dataset Size\": len(documents),\n",
    "            \"Params\": params,\n",
    "            \"Scores\": scores,\n",
    "            \"Computation Time\": computation_time,\n",
    "            \"Topic words\": twords\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "    with open(f\"20NewsGroup_Bertopic_{MODEL_NAME}_{nr_topics-1}.json\", \"w\") as file:\n",
    "        json.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top2Vec supports the following `embedding_model`:\n",
    "- doc2vec\n",
    "- universal-sentence-encoder\n",
    "- universal-sentence-encoder-large\n",
    "- universal-sentence-encoder-multilingual\n",
    "- universal-sentence-encoder-multilingual-large\n",
    "- distiluse-base-multilingual-cased\n",
    "- all-MiniLM-L6-v2\n",
    "- paraphrase-multilingual-MiniLM-L12-v2\n",
    "\n",
    "For those not in the list, Top2Vec also supports callable embedding_model, more information please see (https://top2vec.readthedocs.io/en/latest/api.html). In this example, we build the `embed_with_mpnet` to use the `all-mpnet-base-v2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_with_mpnet(texts):\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "    return model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"umap_args\": {\"n_components\": 5},\n",
    "          \"hdbscan_args\": {'min_cluster_size': 15},\n",
    "          \"min_count\": 50\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_topics in [10]:\n",
    "    results = []\n",
    "    for i in range(3):\n",
    "        print (f\"num_topic: {num_topics}\")\n",
    "        start = time.time()\n",
    "        model = Top2Vec(documents, embedding_model=embed_with_mpnet, **params)\n",
    "        topic_words, _, _ = model.get_topics()\n",
    "\n",
    "        _ = model.hierarchical_topic_reduction(num_topics)\n",
    "        topic_words, _, _ = model.get_topics(num_topics=num_topics, reduced =True)\n",
    "        end = time.time()\n",
    "        computation_time = float(end - start)\n",
    "        twords = [topic_word[:10].tolist() for topic_word in topic_words]\n",
    "        \n",
    "        coherence_metric = TopicCoherence(texts=corpus, topk=10, measure=\"c_npmi\")\n",
    "        coherence_score = coherence_metric.score(twords)\n",
    "\n",
    "        diversity_metric = TopicDiversity(topk=10)\n",
    "        diversity_score = diversity_metric.score(twords)\n",
    "        \n",
    "        scores = {}\n",
    "        scores['npmi'] = coherence_score\n",
    "        scores['diversity'] = diversity_score\n",
    "\n",
    "        result = {\n",
    "            \"Model\": MODEL_NAME,\n",
    "            \"num_topic\": len(topic_words),\n",
    "            \"Dataset Size\": len(documents),\n",
    "            \"Params\": params,\n",
    "            \"Scores\": scores,\n",
    "            \"Computation Time\": computation_time,\n",
    "            \"Topic words\": twords\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "    with open(f\"20NewsGroup_Top2vec_{MODEL_NAME}_{num_topics}.json\", \"w\") as file:\n",
    "        json.dump(results, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we load the LDA model from [OCTIS](https://github.com/MIND-Lab/OCTIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.models.LDA import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nr_topics in [10]:\n",
    "    results = []\n",
    "    for i, random_state in enumerate([0, 21, 42]):\n",
    "        params = {\"random_state\": random_state,\n",
    "                \"alpha\" : \"auto\", \"eta\": \"auto\", \"decay\" : 0.5, \"offset\" : 1.0,\n",
    "                \"iterations\" : 400, \"gamma_threshold\" : 0.001}\n",
    "        \n",
    "        model = LDA(**params, num_topics=nr_topics)\n",
    "        model.use_partitions = False\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        output_tm = model.train_model(dataset)\n",
    "        end = time.time()\n",
    "        computation_time = end - start\n",
    "\n",
    "        twords = output_tm['topics']\n",
    "        \n",
    "        coherence_metric = TopicCoherence(texts=corpus, topk=10, measure=\"c_npmi\")\n",
    "        coherence_score = coherence_metric.score(twords)\n",
    "\n",
    "        diversity_metric = TopicDiversity(topk=10)\n",
    "        diversity_score = diversity_metric.score(twords)\n",
    "        \n",
    "        scores = {}\n",
    "        scores['npmi'] = coherence_score\n",
    "        scores['diversity'] = diversity_score\n",
    "\n",
    "        result = {\n",
    "            \"Dataset Size\": len(documents),\n",
    "            \"Params\": params,\n",
    "            \"Scores\": scores,\n",
    "            \"Computation Time\": computation_time,\n",
    "            \"Topic words\": twords\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "    with open(f\"20NewsGroups_LDA_{nr_topics}.json\", \"w\") as file:\n",
    "        json.dump(results, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopClus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions of [TopClus](https://github.com/yumeng5/TopClus):\n",
    "\n",
    "To execute the code on a new dataset, you need to\n",
    "1. Create a directory named your_dataset under datasets.\n",
    "2. Prepare a text corpus texts.txt (one document per line) under your_dataset as the target corpus for topic discovery.\n",
    "3. Run src/trainer.py with appropriate command line arguments (the default values are usually good start points).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this example, I create a directory 20NewsGroup and store the texts.txt under it. I use the following script to run the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "clusters=(10 20 30)\n",
    "\n",
    "# Loop over the cluster values\n",
    "for n_clusters in \"${clusters[@]}\"; do\n",
    "    python src/trainer.py --dataset 20NewsGroup --n_clusters \"$n_clusters\" --lr 5e-4 --cluster_weight 0.1 --seed 42 --do_cluster --do_inference\n",
    "done\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you get the results, you can use the following script to evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_twords(filename):\n",
    "    result_dict = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            # Remove any leading/trailing whitespace characters\n",
    "            line = line.strip()\n",
    "            if line:  # If the line is not empty\n",
    "                # Split the line into key and values part\n",
    "                key, values = line.split(':')\n",
    "                # Split the values by comma and strip any extra whitespace\n",
    "                value_list = [value.strip() for value in values.split(',')]\n",
    "                # Add the key and list of values to the dictionary\n",
    "                result_dict[key] = value_list\n",
    "    return result_dict\n",
    "\n",
    "results = []\n",
    "for nr_topics in [10, 20, 30]:\n",
    "    twords_dic = read_twords(f'your_file_path/topics_final_{nr_topics}.txt')\n",
    "    twords = list(twords_dic.values())\n",
    "    coherence_metric = TopicCoherence(texts=corpus, topk=10, measure=\"c_npmi\")\n",
    "    umass = TopicCoherence(texts=corpus, topk=10, measure='u_mass')\n",
    "    uci = TopicCoherence(texts=corpus, topk=10, measure='c_uci')\n",
    "    \n",
    "    coherence_score = coherence_metric.score(twords)\n",
    "    umass_score = umass.score(twords)\n",
    "    uci_score = uci.score(twords)\n",
    "  \n",
    "     \n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    diversity_score = diversity_metric.score(twords)\n",
    "\n",
    "    scores = {}\n",
    "    scores['npmi'] = coherence_score\n",
    "    scores['diversity'] = diversity_score\n",
    "    scores['umass'] = umass_score\n",
    "    scores['uci'] = uci_score\n",
    "\n",
    "    result = {\n",
    "        \"Dataset Size\": len(corpus),\n",
    "        \"nr_topics\" : nr_topics,\n",
    "        \"Scores\": scores,\n",
    "        \"Topic words\": twords\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "\n",
    "with open(\"Elonmusk_Pre_TopClus.json\", \"w\") as file:\n",
    "    json.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
